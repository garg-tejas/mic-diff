data:
  dataset: "APTOS"
  seed: 2000
  label_min_max: [0.001, 0.999]
  num_classes: 5
  num_workers: 2
  dataroot: "/kaggle/input/aptos2019/"
  traindata: "/kaggle/working/dataset/APTOS2019/aptos_train.pkl"
  valdata: "/kaggle/working/dataset/APTOS2019/aptos_val.pkl"
  testdata: "/kaggle/working/dataset/APTOS2019/aptos_test.pkl"
  train_csv: "/kaggle/input/aptos2019/train_1.csv"
  val_csv: "/kaggle/input/aptos2019/valid.csv"
  test_csv: "/kaggle/input/aptos2019/test.csv"
  train_images: "/kaggle/input/aptos2019/train_images/train_images"
  val_images: "/kaggle/input/aptos2019/val_images/val_images"
  test_images: "/kaggle/input/aptos2019/test_images/test_images"

model:
  type: "simple"
  data_dim: 150528
  n_input_channels: 3
  n_input_padding: 0
  feature_dim: 6144
  hidden_dim: 6144
  cat_x: True
  cat_y_pred: True
  arch: resnet18
  var_type: fixedlarge
  ema_rate: 0.9999
  ema: True
  num_k: 6

diffusion:
  beta_schedule: linear
  beta_start: 0.0001
  beta_end: 0.02
  timesteps: 1000
  test_timesteps: 60
  ddim_steps: 10
  vis_step: 100
  num_figs: 10
  include_guidance: True
  apply_aux_cls: True
  aux_cls:
    arch: resnet18_ckpt
    pre_train: True
    joint_train: True
    logging_interval: 1

training:
  batch_size: 1
  n_epochs: 1000
  warmup_epochs: 2
  add_t0_loss: False
  n_steps_req_grad: 100
  n_minibatches_add_ce: 20
  n_ce_epochs_warmup: 5
  n_ce_epochs_interval: 25
  n_sanity_check_epochs_freq: 500
  snapshot_freq: 1000000000
  logging_freq: 200
  validation_freq: 10
  image_folder: "training_image_samples"
  gradient_accumulation_steps: 64

sampling:
  batch_size: 1
  sampling_size: 100
  last_only: True
  image_folder: "sampling_image_samples"

testing:
  batch_size: 1
  sampling_size: 100
  last_only: True
  plot_freq: 50
  image_folder: "testing_image_samples"
  n_samples: 50
  n_bins: 10
  compute_metric_all_steps: False
  metrics_t: 0
  ttest_alpha: 0.05
  trimmed_mean_range: [0.0, 100.0]
  PICP_range: [2.5, 97.5]
  make_plot: True
  squared_plot: True
  plot_true: True
  plot_gen: True
  fig_size: [6, 4]

optim:
  weight_decay: 0.0001
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: False
  eps: 0.00000001
  grad_clip: 1.0
  lr_schedule: True
  min_lr: 0.00001

aux_optim:
  weight_decay: 0.0001
  optimizer: "Adam"
  lr: 0.0002
  beta1: 0.9
  amsgrad: True
  eps: 0.00000001
  grad_clip: 1.0

dcg_pretraining:
  n_epochs: 100
  batch_size: 1
  gradient_accumulation_steps: 64
  lr: 2e-4
  optimizer: "Adam"
  weight_decay: 0.0001
  beta1: 0.9
  amsgrad: False
  eps: 0.00000001
  grad_clip: 1.0
